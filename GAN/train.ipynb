{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import CelebALoad\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "root_dir = '../dataset/images'\n",
    "image_list = os.listdir(root_dir)\n",
    "\n",
    "dataset = CelebALoad(root_dir, image_list, resize=(64,64))\n",
    "dataloader = DataLoader(dataset, batch_size=32, pin_memory=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from gan import Generator, Discriminator\n",
    "from torch import nn\n",
    "\n",
    "Z_DIM = 256\n",
    "G = Generator(z_dim=Z_DIM, img_channels=3, feature_g=256)\n",
    "D = Discriminator(img_channels=3, feature_d=256)\n",
    "G = G.to(\"cuda\")\n",
    "D = D.to(\"cuda\")\n",
    "beta1 = 0.5\n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr=1e-4, betas=(beta1, 0.999))\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr=4e-4, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 64])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "   ConvTranspose2d-1           [-1, 2048, 4, 4]       8,390,656\n",
      "              ReLU-2           [-1, 2048, 4, 4]               0\n",
      "   ConvTranspose2d-3           [-1, 1024, 8, 8]      33,555,456\n",
      "              ReLU-4           [-1, 1024, 8, 8]               0\n",
      "   ConvTranspose2d-5          [-1, 512, 16, 16]       8,389,120\n",
      "              ReLU-6          [-1, 512, 16, 16]               0\n",
      "   ConvTranspose2d-7          [-1, 256, 32, 32]       2,097,408\n",
      "              ReLU-8          [-1, 256, 32, 32]               0\n",
      "   ConvTranspose2d-9            [-1, 3, 64, 64]          12,291\n",
      "             Tanh-10            [-1, 3, 64, 64]               0\n",
      "================================================================\n",
      "Total params: 52,444,931\n",
      "Trainable params: 52,444,931\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 7.69\n",
      "Params size (MB): 200.06\n",
      "Estimated Total Size (MB): 207.75\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "with torch.no_grad():\n",
    "    dummy_input = torch.randn(1, Z_DIM, 1, 1).to(\"cuda\")\n",
    "    output = G(dummy_input)\n",
    "\n",
    "print(output.shape)\n",
    "summary(G, (Z_DIM, 1, 1),device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 256, 32, 32]          12,544\n",
      "         LeakyReLU-2          [-1, 256, 32, 32]               0\n",
      "            Conv2d-3          [-1, 512, 16, 16]       2,097,664\n",
      "         LeakyReLU-4          [-1, 512, 16, 16]               0\n",
      "            Conv2d-5           [-1, 1024, 8, 8]       8,389,632\n",
      "         LeakyReLU-6           [-1, 1024, 8, 8]               0\n",
      "            Conv2d-7           [-1, 2048, 4, 4]      33,556,480\n",
      "         LeakyReLU-8           [-1, 2048, 4, 4]               0\n",
      "            Conv2d-9              [-1, 1, 1, 1]          32,769\n",
      "================================================================\n",
      "Total params: 44,089,089\n",
      "Trainable params: 44,089,089\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 7.50\n",
      "Params size (MB): 168.19\n",
      "Estimated Total Size (MB): 175.73\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    dummy_input = torch.randn(1, 3, 64, 64).to(\"cuda\")\n",
    "    output = D(dummy_input)\n",
    "print(output.shape)\n",
    "summary(D, (3, 64, 64),device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# Convert and save\n",
    "\n",
    "def show_generated_images(fake_images, epoch, batch):\n",
    "    fake_images = fake_images[:32].detach().cpu().copy()\n",
    "    grid = vutils.make_grid(fake_images, normalize=True, nrow=8)\n",
    "    img = grid.permute(1, 2, 0).numpy()\n",
    "    img = ((img)* 255).astype('uint8') \n",
    "    img = Image.fromarray(img)\n",
    "    img.save(\"generated_images/output_image.png\")\n",
    "    if batch%50 == 0:\n",
    "        img.save(f\"generated_images/e{epoch}_b{batch}.png\")\n",
    "    del fake_images, grid, img, batch, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import psutil\n",
    "def print_largest_variables(n=5):\n",
    "    \"\"\"Prints top `n` largest variables in RAM with their names (if possible).\"\"\"\n",
    "    all_objects = gc.get_objects()\n",
    "    size_by_name = defaultdict(list)\n",
    "    \n",
    "    # Track variable names by object identity\n",
    "    for obj in all_objects:\n",
    "        try:\n",
    "            size = sys.getsizeof(obj)\n",
    "            referrers = gc.get_referrers(obj)\n",
    "            for ref in referrers:\n",
    "                # Check if the object is in globals()/locals()\n",
    "                if isinstance(ref, dict):\n",
    "                    for name, val in ref.items():\n",
    "                        if val is obj:\n",
    "                            size_by_name[(name, type(obj))].append(size)\n",
    "                            break\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Aggregate sizes per (name, type)\n",
    "    aggregated = []\n",
    "    for (name, typ), sizes in size_by_name.items():\n",
    "        aggregated.append((sum(sizes), name, typ))\n",
    "    \n",
    "    # Sort by total size (descending)\n",
    "    aggregated.sort(reverse=True, key=lambda x: x[0])\n",
    "    \n",
    "    print(f\"Top {n} largest variables in RAM:\")\n",
    "    for i, (size, name, typ) in enumerate(aggregated[:n], 1):\n",
    "        print(f\"{i}. {name} ({typ}): {size / (1024 ** 2):.2f} MB\") \n",
    "         \n",
    "def cleanup_memory():\n",
    "    gc.collect()  # Force Python garbage collection\n",
    "    torch.cuda.empty_cache()  # Clear GPU memory (if using CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from IPython.display import clear_output\n",
    "from torch.autograd import grad\n",
    "import torch\n",
    "from time import time, strftime, localtime\n",
    "import cv2\n",
    "import logging as log\n",
    "\n",
    "# WGAN-GP Gradient Penalty\n",
    "def gradient_penalty(D, real_images, fake_images, device=\"cuda\"):\n",
    "        \n",
    "    batch_size, channels, height, width = real_images.shape\n",
    "    alpha = torch.rand(batch_size, 1, 1, 1).to(device)  # Random interpolation\n",
    "    interpolated_images = alpha * real_images+ (1 - alpha) * fake_images\n",
    "    interpolated_images.requires_grad = True\n",
    "\n",
    "    # Calculate the gradient\n",
    "    d_interpolated = D(interpolated_images)\n",
    "    gradients = grad(\n",
    "        outputs=d_interpolated, \n",
    "        inputs=interpolated_images, \n",
    "        grad_outputs=torch.ones(d_interpolated.size()).to(device),\n",
    "        create_graph=True, \n",
    "        retain_graph=True, \n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    \n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    grad_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()  # Penalty term\n",
    "    del alpha, interpolated_images, d_interpolated\n",
    "    del gradients\n",
    "    return grad_penalty\n",
    "\n",
    "def save_best_model(g_loss, best_G_loss):\n",
    "    if g_loss < best_G_loss:\n",
    "\n",
    "        torch.save(D.state_dict(), 'models/best_model_D.pth')\n",
    "        best_G_loss = g_loss\n",
    "        torch.save(G.state_dict(), 'models/best_model_G.pth')\n",
    "        print(f\"âœ… Saved best model with g_loss: {g_loss:.4f}\")\n",
    "    return best_G_loss\n",
    "\n",
    "def denormalize(tensor, mean, std):\n",
    "    mean = torch.tensor(mean, device=tensor.device).view(3, 1, 1)\n",
    "    std = torch.tensor(std, device=tensor.device).view(3, 1, 1)\n",
    "    return tensor * std + mean\n",
    "\n",
    "def save_sample(fake_imgs, epoch, batch):\n",
    "    grid = vutils.make_grid(fake_imgs.detach(), nrow=8)  # No need for normalize=False\n",
    "    grid = denormalize(grid, [0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Denormalize the [-1, 1] range\n",
    "    img = grid.clamp(0, 1).mul(255).permute(1, 2, 0).contiguous().cpu().numpy().astype(np.uint8)\n",
    "    img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(f\"generated_images/e{epoch}_b{batch}.png\", img_bgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(real_imgs, epochs, epoch, batch, best_G_loss, d_loss_record, g_loss_record, writer, train_d,):\n",
    "    log.debug(f\"RAM Used 1: {psutil.virtual_memory().used / (1024 ** 3):.2f} GB\")\n",
    "    cleanup_memory()\n",
    "    G.train()\n",
    "    D.train()\n",
    "    batch_size = real_imgs.size(0)\n",
    "    ### Train Discriminator ###\n",
    "    if train_d :    \n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(batch_size, Z_DIM, 1, 1, device=\"cuda\")\n",
    "            fake_imgs = G(z).detach().clone()\n",
    "            del z\n",
    "        log.debug(f\"RAM Used 2: {psutil.virtual_memory().used / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "        # WGAN loss for Discriminator\n",
    "        d_loss_real = torch.mean(D(real_imgs))   # Expect real images to have high score\n",
    "        d_loss_fake = torch.mean(D(fake_imgs))   # Expect fake images to have low score\n",
    "        d_loss = d_loss_fake - d_loss_real\n",
    "        gp = gradient_penalty(D, real_imgs, fake_imgs, device=\"cuda\")\n",
    "        lambda_gp = 20  # Typically set to 10, increased to 20 after trial and error (the disc learns fast but not adapting)\n",
    "        d_loss += lambda_gp * gp\n",
    "        log.debug(f\"RAM Used 4: {psutil.virtual_memory().used / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer_D.zero_grad()\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        d_loss_record= d_loss.detach().item()\n",
    "        log.debug(f\"RAM Used 5: {psutil.virtual_memory().used / (1024 ** 3):.2f} GB\")\n",
    "        del gp\n",
    "        del real_imgs, fake_imgs\n",
    "    ### END Train Discriminator ###\n",
    "    \n",
    "    ### Train Generator ###\n",
    "    if batch%5 == 0:\n",
    "\n",
    "        z = torch.randn(batch_size, Z_DIM, 1, 1, device=\"cuda\")\n",
    "        fake_imgs = G(z)\n",
    "        pred = D(fake_imgs)  \n",
    "\n",
    "        log.debug(f\"RAM Used 6: {psutil.virtual_memory().used / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "        g_loss = -torch.mean(pred)  # fool discriminator\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        g_loss_record = g_loss.detach().item()\n",
    "        best_G_loss = save_best_model(g_loss.item(), best_G_loss)\n",
    "        log.debug(f\"RAM Used 7: {psutil.virtual_memory().used / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "        del pred, z, g_loss\n",
    "        log.debug(f\"RAM Used 8: {psutil.virtual_memory().used / (1024 ** 3):.2f} GB\")\n",
    "    \n",
    "    ### END Train Generator ###\n",
    "    \n",
    "    ### Tensorboard and results\n",
    "    if batch % 10 == 0:\n",
    "        writer.add_scalars( \"gan/losses\", {\n",
    "                            \"D_loss\": float(d_loss_record),\n",
    "                            \"G_loss\": float(g_loss_record)\n",
    "                        }, batch + epoch * len(dataloader))\n",
    "        writer.flush()  # Ensure data is written to disk\n",
    "        with torch.no_grad():\n",
    "            save_sample(fake_imgs, epoch, batch)\n",
    "\n",
    "    if batch == len(dataloader):\n",
    "        torch.save(D.state_dict(), f'models/model_{epoch}_D.pth')\n",
    "        torch.save(G.state_dict(), f'models/model_{epoch}_G.pth')\n",
    "    \n",
    "    log.debug(f\"RAM Used 9: {psutil.virtual_memory().used / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "    \n",
    "    clear_output(wait = True)\n",
    "\n",
    "    batch += 1\n",
    "\n",
    "    cleanup_memory()\n",
    "    log.debug(f\"RAM Used 10: {psutil.virtual_memory().used / (1024 ** 3):.2f} GB\")\n",
    "    print(\n",
    "        \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "        % (epoch, epochs, batch, len(dataloader), d_loss_record, g_loss_record)\n",
    "    )\n",
    "    return batch, best_G_loss, d_loss_record, g_loss_record \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, epoch, batch, best_G_loss, d_loss_record, g_loss_record, writer, train_d = True):\n",
    "    # To let the stack delete on each iteration\n",
    "    for real_imgs in dataloader:\n",
    "        real_imgs = real_imgs.to(\"cuda\")\n",
    "        batch, best_G_loss, d_loss_record, g_loss_record = train_batch(real_imgs, epochs, epoch, batch, best_G_loss, d_loss_record, g_loss_record, writer, train_d=True)\n",
    "    return best_G_loss, d_loss_record, g_loss_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5/5] [Batch 6333/6332] [D loss: -15.845795] [G loss: -6.875937]\n"
     ]
    }
   ],
   "source": [
    "# -500 cutoff is picked after watching logs\n",
    "log.basicConfig(level=log.INFO)\n",
    "\n",
    "try:\n",
    "    now = localtime(time())\n",
    "    now = strftime(\"%m-%d_%H_%M\", now)\n",
    "    writer = SummaryWriter(f\"Logs/{now}/\")\n",
    "    best_G_loss = float(\"inf\")\n",
    "    d_loss_record = 0\n",
    "    g_loss_record = 0\n",
    "    epochs = 5\n",
    "    for epoch in range(1, epochs+1):\n",
    "        batch = 1\n",
    "        print(\"Training...\")\n",
    "        critic_n = 5\n",
    "        best_G_loss, d_loss_record, g_loss_record = train(epochs, epoch, batch, best_G_loss, d_loss_record, g_loss_record, writer)\n",
    "        cleanup_memory()\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training stopped by user.\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10/10] [Batch 6333/6332] [D loss: -12.857266] [G loss: -1.324729]\n"
     ]
    }
   ],
   "source": [
    "# -500 cutoff is picked after watching logs\n",
    "log.basicConfig(level=log.INFO)\n",
    "\n",
    "try:\n",
    "    epochs = 10\n",
    "    for epoch in range(1, epochs+1):\n",
    "        batch = 1\n",
    "        print(\"Training...\")\n",
    "        critic_n = 5\n",
    "        best_G_loss, d_loss_record, g_loss_record = train(epochs, epoch, batch, best_G_loss, d_loss_record, g_loss_record, writer)\n",
    "        cleanup_memory()\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training stopped by user.\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(D.state_dict(), 'models/final_D.pth')\n",
    "torch.save(G.state_dict(), 'models/final_G.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
